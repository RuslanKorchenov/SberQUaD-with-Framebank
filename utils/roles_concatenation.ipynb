{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3f6e418d-cedb-49b2-b10c-9ee72e905c23",
   "metadata": {},
   "source": [
    "# Добавление ролей в исходный набор данных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f440101-afde-4c52-aa12-cc649c8cb490",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "with open('train.csv','r') as csvin, open('train.tsv', 'w') as tsvout:\n",
    "    csvin = csv.reader(csvin)\n",
    "    tsvout = csv.writer(tsvout, delimiter='\\t')\n",
    "\n",
    "    for row in csvin:\n",
    "        tsvout.writerow(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8841ae68-9776-4745-91a3-4a283eacad95",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('dev.csv','r') as csvin, open('dev.tsv', 'w') as tsvout:\n",
    "    csvin = csv.reader(csvin)\n",
    "    tsvout = csv.writer(tsvout, delimiter='\\t')\n",
    "\n",
    "    for row in csvin:\n",
    "        tsvout.writerow(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "aecf68d0-062a-48e5-bf38-d40a9e721883",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "train_framebank = np.load('train_framebank.npy', allow_pickle=True)\n",
    "dev_framebank = np.load('dev_framebank.npy', allow_pickle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "21104f1c-dd6e-4d36-a874-e7469f563950",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_roles(lemma, role_annot, f):\n",
    "    for sent_num, ann_sent in enumerate(role_annot):\n",
    "      # print('sent_num: {}'.format(sent_num))\n",
    "      # print('ann_sent: {}'.format(ann_sent))\n",
    "        for event in ann_sent:\n",
    "            f.write('=====Pred: {}\\n'.format(lemma[sent_num][event.pred[0]]))\n",
    "            for arg in event.args:\n",
    "                f.write('Arg({}): {}\\n'.format(arg.tag, lemma[sent_num][arg.begin]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9120055d-bf3b-400c-964e-668fcb240b20",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 45328/45328 [00:02<00:00, 15582.48it/s]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "f = open('train_roles.txt', 'w')\n",
    "\n",
    "for temp in tqdm(train_framebank):\n",
    "  context = temp['context']\n",
    "  question = temp['question']\n",
    "  f.write(context['text'] +'\\n')\n",
    "  for t in context['lemma']:\n",
    "    for t1 in t:\n",
    "      f.write(t1 + ' ')\n",
    "  f.write('\\n')\n",
    "  print_roles(context['lemma'], context['srl'], f)\n",
    "  f.write('\\n')\n",
    "  f.write(question['text'] + '\\n')\n",
    "  for t in question['lemma']:\n",
    "    for t1 in t:\n",
    "      f.write(t1 + ' ')\n",
    "  f.write('\\n')   \n",
    "  print_roles(question['lemma'], question['srl'], f)\n",
    "  f.write('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8de5aab-9ee6-491d-ad04-95ab3c13a7f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open('dev_roles.txt', 'w')\n",
    "\n",
    "for temp in tqdm(dev_framebank):\n",
    "  context = temp['context']\n",
    "  question = temp['question']\n",
    "  f.write(context['text'] +'\\n')\n",
    "  for t in context['lemma']:\n",
    "    for t1 in t:\n",
    "      f.write(t1 + ' ')\n",
    "  f.write('\\n')\n",
    "  print_roles(context['lemma'], context['srl'], f)\n",
    "  f.write('\\n')\n",
    "  f.write(question['text'] + '\\n')\n",
    "  for t in question['lemma']:\n",
    "    for t1 in t:\n",
    "      f.write(t1 + ' ')\n",
    "  f.write('\\n')   \n",
    "  print_roles(question['lemma'], question['srl'], f)\n",
    "  f.write('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1fa40ffc-ec61-4f26-b3e0-16f0a66fb5e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import numpy\n",
    "import csv\n",
    "\n",
    "framebank_file = 'train_roles.txt'\n",
    "# framebank_file = 'dev_roles.txt'\n",
    "\n",
    "def substring_after(s, delim):\n",
    "    return s.partition(delim)[2]\n",
    "\n",
    "def create_roles_dictionary(framebank_file=framebank_file):\n",
    "    with open(framebank_file, encoding=\"utf8\") as fbFile:\n",
    "        # Удалить пробелы и переносы строк в выделенном pred/arg\n",
    "        regexp = \"^\\s+|\\n|\\r|\\s+$\"\n",
    "        pred_delimeter = '=====Pred: '\n",
    "        arg_delim = ': '\n",
    "        arg_role_start_char = '('\n",
    "        arg_role_end_char = ')'\n",
    "\n",
    "        current_line = 1 \n",
    "        # строка с предложением которое анализируем\n",
    "        current_phrase = ''\n",
    "        # выделененные после анализа роли\n",
    "        roles = \"\"\n",
    "        phrases_roles = {}\n",
    "\n",
    "        tokenized_phr_line = 2\n",
    "        roles_in_phrase_map = {}\n",
    "        phrases_tokenized_ph = {}\n",
    "        roles_in = {}\n",
    "        \n",
    "        pred_cnt = 1\n",
    "        pred_num = 0\n",
    "        for num, line in enumerate(fbFile, 1):\n",
    "            if num == current_line:\n",
    "                if pred_delimeter in line:\n",
    "                    if pred_cnt == 1 and pred_num > 0:\n",
    "                      roles_in[pred_num] =  roles_in_phrase_map\n",
    "                      pred_cnt = 0\n",
    "                      roles_in_phrase_map = {}\n",
    "\n",
    "                    pred = re.sub(regexp, '', substring_after(line, pred_delimeter))\n",
    "                    roles_in_phrase_map[pred] = '<pred>'\n",
    "                    # print('pred: ', pred)\n",
    "                    pred_num += 1\n",
    "                    pred_cnt = 1\n",
    "                    current_line += 1\n",
    "                    continue\n",
    "                elif 'Arg(' in line:\n",
    "                    arg = re.sub(regexp, '', substring_after(line, arg_delim))\n",
    "                    arg_role = line[line.find(arg_role_start_char) + 1 : line.find(arg_role_end_char)]\n",
    "                    roles += arg_role + ','\n",
    "                    roles_in_phrase_map[arg] = arg_role\n",
    "                    # print('ARG: ', arg)\n",
    "                    # print('ARG role: ', arg_role)\n",
    "                    current_line += 1\n",
    "                    continue          \n",
    "                elif line == '\\n':\n",
    "                    roles_in[pred_num] =  roles_in_phrase_map\n",
    "                    phrases_roles[current_phrase] = roles_in                    \n",
    "                    roles_in_phrase_map = {}\n",
    "                    roles_in = {}\n",
    "                    pred_cnt = 0\n",
    "                    pred_num = 0\n",
    "                    current_line += 1\n",
    "                    continue\n",
    "\n",
    "                tokenized_phr_line = current_line + 1\n",
    "                current_line += 2\n",
    "                current_phrase = line.replace(\"\\n\",\"\")\n",
    "\n",
    "            if num == tokenized_phr_line:\n",
    "                phrases_tokenized_ph[current_phrase] = line.replace(\"\\n\",\"\")\n",
    "\n",
    "    return phrases_roles, phrases_tokenized_ph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b43c084b-648e-4cca-87ae-b03135b3ee8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "roles_dictionary, tokenized_ph_dictionary = create_roles_dictionary()\n",
    "\n",
    "def align_tokens_lenght(tokenized_phrase, phrase_roles):\n",
    "    tokens = tokenized_phrase.split()\n",
    "    roles_tokens = []\n",
    "\n",
    "    for token in tokens:\n",
    "      temp = 1\n",
    "      for phrases in phrase_roles:\n",
    "          if token in phrase_roles[phrases]:\n",
    "              roles_tokens.append(phrase_roles[phrases][token])\n",
    "              del phrase_roles[phrases][token]\n",
    "              temp = 0\n",
    "              break\n",
    "\n",
    "      if temp == 1:\n",
    "        roles_tokens.append('<unk>')\n",
    "    return roles_tokens\n",
    "\n",
    "\n",
    "def read_tsv_and_find_roles(file, writer):\n",
    "    file_reader = csv.DictReader(file, delimiter='\\t', quoting=csv.QUOTE_NONE)\n",
    "    line_count = 0\n",
    "\n",
    "    for num, row in enumerate(file_reader):\n",
    "        if line_count == 0:\n",
    "            # print(f'Column names are {\" \".join(row)}')\n",
    "            line_count += 1\n",
    "\n",
    "        ph1 = row[\"context\"]\n",
    "        ph2 = row[\"question\"]\n",
    "        \n",
    "        try:\n",
    "            tokenized_ph1 = tokenized_ph_dictionary[ph1]\n",
    "        except KeyError as e:\n",
    "            print(num)\n",
    "            # print(e)\n",
    "            # print(ph1[len(ph1) - 1:])\n",
    "            # print(ph1[:-1])\n",
    "            # print(ph1[len(ph1) - 1:] == ' ' or ph1[len(ph1) - 1:] == '\\t')\n",
    "            if ph1[len(ph1) - 1:] == ' ' or ph1[len(ph1) - 1:] == '\\t':\n",
    "                tokenized_ph1 = tokenized_ph_dictionary[ph1[:-1]]            \n",
    "            else:\n",
    "                continue\n",
    "\n",
    "        try:\n",
    "            tokenized_ph2 = tokenized_ph_dictionary[ph2]\n",
    "        except KeyError as e:\n",
    "            # print(e)\n",
    "            # print(ph2[len(ph2) - 1:])\n",
    "            # print(ph2[:-1])\n",
    "            # print(ph2[len(ph2) - 1:] == ' ' or ph2[len(ph2) - 1:] == '\\t')\n",
    "            if ph2[len(ph2) - 1:] == ' ' or ph2[len(ph2) - 1:] == '\\t':\n",
    "                tokenized_ph2 = tokenized_ph_dictionary[ph2[:-1]]            \n",
    "            else:\n",
    "                continue\n",
    "\n",
    "        roles1 = align_tokens_lenght(tokenized_ph1, roles_dictionary[ph1])\n",
    "        try:\n",
    "          roles2 = align_tokens_lenght(tokenized_ph2, roles_dictionary[ph2])        \n",
    "        except KeyError as e:\n",
    "          print(e)\n",
    "          continue\n",
    "        # ','.join() - convert roles_tokens list to string with ',' separator\n",
    "        writer.writerow({\n",
    "            'context': tokenized_ph1, \n",
    "            'context_roles': ','.join(roles1),\n",
    "            'question': tokenized_ph2,\n",
    "            'question_roles': ','.join(roles2), \n",
    "            'answer': row[\"answer\"],\n",
    "            'answer_start': row[\"answer_start\"]\n",
    "        })\n",
    "\n",
    "        line_count += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1245dd29-fba2-4612-930e-b406af775e86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2479\n",
      "3874\n",
      "22051\n",
      "27159\n",
      "28149\n",
      "33954\n",
      "35228\n",
      "37003\n"
     ]
    }
   ],
   "source": [
    "test_file = 'train.tsv'\n",
    "output_file = 'train_roles.tsv'\n",
    "\n",
    "def convert_tsv_data_to_with_role(train_file=test_file, output_file=output_file):\n",
    "    with open(output_file, mode='w', encoding='utf-8', newline='') as output_file:\n",
    "        fieldnames = ['context','context_roles','question','question_roles','answer', 'answer_start']\n",
    "        delimiter = '\\t'\n",
    "        quotechar = ''        \n",
    "        quoting = quoting=csv.QUOTE_NONE\n",
    "        output_writer = csv.DictWriter(output_file, fieldnames=fieldnames, delimiter=delimiter, quoting=quoting, quotechar=quotechar, escapechar='')\n",
    "        output_writer.writeheader()\n",
    "\n",
    "        with open(train_file, mode='r', encoding='utf-8') as tsv_file:\n",
    "            read_tsv_and_find_roles(tsv_file,output_writer)\n",
    "\n",
    "convert_tsv_data_to_with_role()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c24f50d-146b-4b4e-b91f-cf8417175bf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_file = 'dev.tsv'\n",
    "output_file = 'dev_roles.tsv'\n",
    "\n",
    "def convert_tsv_data_to_with_role(train_file=test_file, output_file=output_file):\n",
    "    with open(output_file, mode='w', encoding='utf-8', newline='') as output_file:\n",
    "        fieldnames = ['context','context_roles','question','question_roles','answer', 'answer_start']\n",
    "        delimiter = '\\t'\n",
    "        quotechar = ''        \n",
    "        quoting = quoting=csv.QUOTE_NONE\n",
    "        output_writer = csv.DictWriter(output_file, fieldnames=fieldnames, delimiter=delimiter, quoting=quoting, quotechar=quotechar, escapechar='')\n",
    "        output_writer.writeheader()\n",
    "\n",
    "        with open(train_file, mode='r', encoding='utf-8') as tsv_file:\n",
    "            read_tsv_and_find_roles(tsv_file,output_writer)\n",
    "\n",
    "convert_tsv_data_to_with_role()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
